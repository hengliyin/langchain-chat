import { LLMService } from "../services/llm.service.js";
import { asyncHandler } from "../utils/asyncHandler.js";
import { successResponse } from "../utils/response.js";
import fs from 'fs';
import path from 'path';

const llmService = new LLMService();

// åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
const logFile = path.join(process.cwd(), 'logs', 'chat-debug.log');

// ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
const logsDir = path.dirname(logFile);
if (!fs.existsSync(logsDir)) {
  fs.mkdirSync(logsDir, { recursive: true });
}

// å†™å…¥æ—¥å¿—çš„è¾…åŠ©å‡½æ•°
function writeLog(message) {
  const timestamp = new Date().toISOString();
  const logMessage = `[${timestamp}] ${message}\n`;
  console.log(logMessage);
  fs.appendFileSync(logFile, logMessage);
}

export const chat = asyncHandler(async (req, res) => {
  const { message, model, temperature, history } = req.body;
  
  // æ—¥å¿—: æ‰“å°æŽ¥æ”¶åˆ°çš„å‚æ•°
  writeLog('ðŸ“¨ Chat è¯·æ±‚æŽ¥æ”¶:');
  writeLog(`  - æ¶ˆæ¯: "${message ? message.substring(0, 50) : '(ç©º)'}"${message && message.length > 50 ? '...' : ''}`);
  writeLog(`  - åŽ†å²æ¶ˆæ¯æ•°: ${Array.isArray(history) ? history.length : 0}`);
  if (Array.isArray(history) && history.length > 0) {
    history.forEach((h, idx) => {
      writeLog(`    [${idx}] ${h.role}: "${h.content.substring(0, 30)}..."`);
    });
  }
  
  const result = await llmService.chat(message, { 
    model, 
    temperature: temperature ? parseFloat(temperature) : undefined,
    history: history || []
  });
  res.json(successResponse(result, "Chat completed successfully"));
});

export const streamChat = asyncHandler(async (req, res) => {
  const { message, model, history } = req.body;
  
  // æ—¥å¿—: æ‰“å°æŽ¥æ”¶åˆ°çš„å‚æ•°
  writeLog('ðŸ“¨ Stream Chat è¯·æ±‚æŽ¥æ”¶:');
  writeLog(`  - æ¶ˆæ¯: "${message ? message.substring(0, 50) : '(ç©º)'}"${message && message.length > 50 ? '...' : ''}`);
  writeLog(`  - åŽ†å²æ¶ˆæ¯æ•°: ${Array.isArray(history) ? history.length : 0}`);
  if (Array.isArray(history) && history.length > 0) {
    history.forEach((h, idx) => {
      writeLog(`    [${idx}] ${h.role}: "${h.content.substring(0, 30)}..."`);
    });
  }
  
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  res.setHeader('X-Accel-Buffering', 'no');

  try {
    const stream = llmService.streamChat(message, { 
      model,
      history: history || []
    });
    for await (const chunk of stream) {
      res.write(`data: ${JSON.stringify({ content: chunk })}\n\n`);
    }
    res.write('data: [DONE]\n\n');
    res.end();
  } catch (error) {
    res.write(`data: ${JSON.stringify({ error: error.message })}\n\n`);
    res.end();
  }
});
